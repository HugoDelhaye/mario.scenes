"""
Data Loading Utilities for Mario Scenes

This module provides functions to load scene metadata, annotations, background images,
and dimensionality reduction results for Super Mario Bros level analysis.

Main Functions:
    - load_scenes_info(): Load scene boundary definitions from mastersheet
    - load_annotation_data(): Load curated feature annotations for all scenes
    - load_background_images(): Load level or scene background images
    - load_reduced_data(): Load dimensionality-reduced scene representations

All functions use centralized paths relative to the package BASE_DIR.
"""

import pandas as pd
import os.path as op
import os
import json
from PIL import Image

BASE_DIR = op.dirname(op.dirname(op.dirname(op.dirname(op.abspath(__file__)))))
SOURCEDATA = op.join(BASE_DIR, 'sourcedata')
SCENES_MASTERSHEET = op.join(SOURCEDATA, 'scenes_info', 'scenes_mastersheet.csv')


def load_scenes_info(format='df'):
    """
    Load scene boundary definitions and layout information.

    Reads the scenes mastersheet containing World, Level, Scene identifiers
    along with Entry/Exit points and Layout IDs for each atomic scene.

    Parameters
    ----------
    format : {'df', 'dict'}, default='df'
        Output format:
        - 'df': Returns pandas DataFrame with columns [World, Level, Scene, Entry point, Exit point, Layout]
        - 'dict': Returns nested dict keyed by scene_id (e.g., 'w1l1s1') with
                  sub-keys: 'start', 'end', 'level_layout'

    Returns
    -------
    pandas.DataFrame or dict
        Scene definitions in requested format.

    Raises
    ------
    AssertionError
        If scenes_mastersheet.csv is not found. Run 'invoke get-scenes-data' first.
    ValueError
        If format is not 'df' or 'dict'.

    Examples
    --------
    >>> scenes_df = load_scenes_info(format='df')
    >>> print(scenes_df.head())

    >>> scenes_dict = load_scenes_info(format='dict')
    >>> print(scenes_dict['w1l1s1'])
    {'start': 0, 'end': 256, 'level_layout': 0}
    """
    # Check if file exists
    assert op.exists(SCENES_MASTERSHEET), f"File not found: {SCENES_MASTERSHEET}, make sure you run 'invoke collect-resources' first."
    
    # Load the data
    scenes_df = pd.read_csv(SCENES_MASTERSHEET)
    if format == 'df':
        return scenes_df
    elif format == 'dict':
        scenes_dict = {}
        for idx, row in scenes_df.iterrows():
            try:
                scene_id = f'w{int(row["World"])}l{int(row["Level"])}s{int(row["Scene"])}'
                scenes_dict[scene_id] = {
                    'start': int(row['Entry point']),
                    'end': int(row['Exit point']),
                    'level_layout': int(row['Layout'])
                }
            except:
                continue
        return scenes_dict
    else:
        raise ValueError('format must be either "df" or "dict"')
    

def load_background_images(level='level'):
    """
    Load background images for levels or scenes as PIL Image objects.

    Background images are generated by averaging pixel columns across multiple
    playthroughs (see make_scene_img.py). They represent canonical visual
    appearance without dynamic sprites.

    Parameters
    ----------
    level : {'level', 'scene'}, default='level'
        Granularity of backgrounds to load:
        - 'level': Full level backgrounds (e.g., 'w1l1.png')
        - 'scene': Individual scene backgrounds (e.g., 'w1l1s1.png')

    Returns
    -------
    dict
        Mapping of image basenames (without .png extension) to PIL Image objects.
        Keys are level/scene IDs like 'w1l1' or 'w1l1s1'.

    Raises
    ------
    FileNotFoundError
        If the background directory doesn't exist. Run 'invoke get-scenes-data' or
        'invoke make-scene-images' to generate backgrounds.

    Examples
    --------
    >>> level_bgs = load_background_images(level='level')
    >>> img = level_bgs['w1l1']
    >>> print(img.size)
    (3392, 224)

    Notes
    -----
    Images are stored in sourcedata/level_backgrounds/ or sourcedata/scene_backgrounds/
    """
    # load images
    if level == 'level':
        folder = 'level_backgrounds'
    if level == 'scene':
        folder = 'scene_backgrounds'

    backgrounds = []
    backgrounds_names = []
    for img in sorted(os.listdir(os.path.join(SOURCEDATA, folder))):
        if img.endswith('.png'):
            backgrounds.append(Image.open(os.path.join(SOURCEDATA, folder, img)))
            backgrounds_names.append(img.split('.')[0])
    # create dict
    backgrounds_dict = {}
    for i, name in enumerate(backgrounds_names):
        backgrounds_dict[name] = backgrounds[i]
    return backgrounds_dict



def load_annotation_data():
    """
    Load curated scene annotation features as a DataFrame.

    Extracts 27 binary feature columns from the scenes mastersheet representing
    gameplay elements present in each scene (enemies, gaps, platforms, etc.).
    Creates scene_ID index for joining with other scene-level data.

    Returns
    -------
    pandas.DataFrame
        Feature matrix with shape (n_scenes, 27) indexed by scene_ID strings.
        Columns represent binary presence/absence of gameplay elements:

        Enemy Features: Enemy, 2-Horde, 3-Horde, 4-Horde
        Terrain: Roof, Gap, Multiple gaps, Variable gaps, Valley types
        Navigation: Stairs, Platforms, Paths
        Special: Risk/Reward, Bonus zone, Flagpole, Beginning

    Examples
    --------
    >>> features = load_annotation_data()
    >>> print(features.loc['w1l1s1'])
    Enemy          0
    2-Horde        0
    Gap            1
    ...

    >>> print(features.shape)
    (210, 27)

    Notes
    -----
    Used as input for dimensionality reduction and clustering analyses.
    See scenes_mastersheet.csv for full feature definitions.
    """
    # Create the 'scene_ID' column
    df = load_scenes_info(format='df')
    df['scene_ID'] = df.apply(
        lambda row: f"w{int(row['World'])}l{int(row['Level'])}s{int(row['Scene'])}",
        axis=1
    )
    
    # List of feature columns to keep (features and identifying variables)
    feature_cols = [
        'Enemy', '2-Horde', '3-Horde', '4-Horde', 'Roof', 'Gap',
        'Multiple gaps', 'Variable gaps', 'Gap enemy', 'Pillar gap', 'Valley',
        'Pipe valley', 'Empty valley', 'Enemy valley', 'Roof valley', '2-Path',
        '3-Path', 'Risk/Reward', 'Stair up', 'Stair down', 'Empty stair valley',
        'Enemy stair valley', 'Gap stair valley', 'Reward', 'Moving platform',
        'Flagpole', 'Beginning', 'Bonus zone'
    ]
    
    annotations_df = df[feature_cols]
    annotations_df.index = df['scene_ID']

    return annotations_df

def load_reduced_data(method='umap'):
    """
    Load 2D dimensionality-reduced scene representations.

    Reads pre-computed low-dimensional embeddings from CSV files generated
    by the dimensionality-reduction task.

    Parameters
    ----------
    method : {'umap', 'pca', 'tsne'}, default='umap'
        Dimensionality reduction method to load:
        - 'umap': UMAP embedding (recommended for visualization)
        - 'pca': PCA projection (linear, interpretable axes)
        - 'tsne': t-SNE embedding (good local structure)

    Returns
    -------
    pandas.DataFrame
        2D coordinates with columns [DR_1, DR_2] indexed by scene_ID.
        Shape: (n_scenes, 2)

    Raises
    ------
    AssertionError
        If the requested method's output file doesn't exist.
        Run 'invoke dimensionality-reduction' to generate embeddings.

    Examples
    --------
    >>> umap_coords = load_reduced_data(method='umap')
    >>> print(umap_coords.head())
                 DR_1      DR_2
    scene_ID
    w1l1s1      -2.34      1.56
    w1l1s2       0.12     -3.21
    ...

    Notes
    -----
    Output files are located in outputs/dimensionality_reduction/{method}.csv
    """
    fname = op.join(BASE_DIR, 'outputs', 'dimensionality_reduction', f'{method}.csv')
    assert op.exists(fname), f"File not found: {fname}, make sure you run 'invoke dimensionality-reduction' first."
    return pd.read_csv(fname, index_col=0)



